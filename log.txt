Thursday 14 June 2018

So this is my attempt at the Thirdlight Go Challenge.  The gist of it is that there are two Go servers:

1. sits and watches a folder, maintains an up-to-date list of its files in it, and communicates any changes to the master server
2. maintains an up-to-date sorted list of all the files in the folders that have watcher servers (1) on them and hands it out as a chunk of json over http when asked

Initial thoughts:
1. it says the master server response should be something like
{“files”:[{“filename”:”cat.jpg”},{“filename”:”dog.jpg”}]}
So is "files" the name of a folder, and cat and dog the contents of 'files'?  This would make sense because if the list was completely flattened then how would you deal with 'folder1/file1' and 'folder2/file1'?  If this is so then you have to go to /foldername to get the contents of foldername, and presumably when you go to / you get the whole bunch of folders.

I have asked Chris about this and he said the intention was for a flat, deduplicated list, but it's fine if I do it with a proper nested structure.

2. What about subfolders?  I suppose you have some nested json to deal with that.

3. presumably the reason for having this sort of setup is that there are very large numbers of files and folders, and lots of requests for the file list, and scanning them all each time there was a request would beat up the hard drive and be very slow

4. If the master server has this big list, say a Go slice, sitting in memory, ready to dish out on request, what happens if it crashes?  The watchers won't know what has happened, presumably, so all the previous data that has been collected up will be thrown away, and when the master starts up again it will be ignorant of all the previous changes.

Possible answers to (4) are:
1. the master periodically saves the file list to disk, so it can pick it up again if it crashes.  'Peridically' would have to mean 'every single time a new file name comes in' to not lose data when there is a crash, and even then, what about the time while it is down?  The file system keeps on changing and when the server comes up again there is no record of the changes while it has been down.
2. The master every now and then does some sort of check with the watchers to make sure that its record matches the ones on the watchers. 

The state I have floating around in the whole system is:
+ a list in each watcher
+ a list in the master

What about if I have immutable state?  With some garbage collection to throw away really old stuff.  So each watcher has a list of all the files in its folder.  BUT every time a file is added/removed, intead of modifying this list, a whole brand-new list is made and the old one is kept.

On the master, it has a copy of the latest list from each of the watchers AND its big new master list.  The job of a watcher is to make sure that the master has a copy of its latest list all the time.  So in principle, whenever it makes a new version of its list, it sends the whole thing over to the master.

In practice, the sending of the whole new list will go something like this:
1. the watcher will ask the master what version of its list that it has
2. the master will respond with
    a. a version number (probably a guid), or
    b. "don't have anything from you"
3. corresponding to the options above:
    a. the watcher will calculate the difference between its latest version and the one that the master has and send over the difference
    b. the watcher will send over its whole latest version

So these are the things the watcher does:
+ keep checking its folder for changes: when one happpens, make a new list of its contents
+ keep pruning out its older versions of the file lists to save space - maybe some rule like:
    - if it's more than a 2 minutes old then throw it away, or
    - only keep the 10 newest copies
+ whenever a new file list is made, send a request to the master (probably an http GET, maybe there is a better way but I don't know it), asking for the latest version that they hold - send them over the difference to the newest one or the whole thing if they don't have it at all.

And these are the things that the master does:
+ maintain a map of lists of file lists: the keys are the watcher guids and the lists are ALL (in theory) the complete sets of file lists that have ever been received from that watcher
+ whenever it gets a request from a watcher saying "which version of my list do you hold?" respond with the guid of the latest version or "I don't have any of your lists"
+ when it gets a complete list from a watcher, add it to their list of versions in the data map
+ when it gets a list diff from a watcher, look up the last version, construct the new one, and stack it in the front of the list
+ keep an eye on the versions in the data struct and keep pruning out the old ones - I'm a bit vague on how to do this - what if someone is putting a new thing into the list of versions at the same time as something else is pruning off the end of it?  I need a sort of fixed-length list that when you put something in, something drops out of the other end.  What if I wrap this up in a GoRoutine, so that it is a ...

Can't my garbage-collector sort-of-thing be a map of file-list guid to file-list, including a time-stamp somewhere.  Then when I collect the garbage, which will be done periodically, I can just loop over the map to find the keys of things that aren't needed any more, and then delete the things from it.  The throw-away rule should be such that I never delete a key that is needed again.  I think I have found the right thing for this: sync map in Go: https://godoc.org/sync#Map .  It says in the docs that the map is optimised for (among other things)  "when multiple goroutines read, write and overwrite entries for disjoint sets of keys ... use of a Map may significantly reduce lock contention compared to a Go map paired with a separate Mutex or RWMutex".  Which was the only alternative I could think of really, use an ordinary map with a Mutex.  The thing is: I need a map that an independent goroutine can sit and look at and dive in and delete old unwanted elements that will never get asked for again.

The master will only ever need the latest version for the lists from the watchers.

To carry on with the things that the master does:

+ remember that in theory, the master keeps all versions of the big master list of files forever - there will just be some discrete garbage collection to throw away old versions and stop memory usage blowing up
+ whenever a new file-watcher list is added, run a function that takes in all the latest file lists from all the servers, deduplicates it, sorts it, and dumps it as a new element in the front of the list of master lists.  This sounds quite slow and memory-greedy but it might be fine, and if not, there are many potential ways of optimising it.
+ listen for requests from the client and hand out the latest version of the master file list when required

I think I now have enough to do the spec.

OK, spec done for now.  The thing that is worrying me at this stage is that this immutable sort of method could quite well have some terrible performance issues.  I think the best strategy is to get it to work and optimise later.

The watcher should do something like this:

func main() {
    state = emptySyncMap
	while True:
       input = getInput(<possibly state here?>)
	   state = updateState(state, input)
	   send_output(state2Output(state))

The trouble with this is that the state gets copied on every single loop, which is a bit of a waste, seeing as on the vast majority of loops nothing will happen.

What about

struct State {
		// some content
}

struct Input {
		// some inputty things
}

func main() {
    state = emptySyncMap
	while True:
       input = getInput(<possibly state here?>)
	   state = updateState(state, input)
	   send_output(stateToOutput(state))
}

func getInput(State) -> Input {
    fileChange = sitAndWaitTillSomethingChangesAndThenTellMe()
	// more things
}

But the trouble with this is that something else could change while you do the 'more things'. 

Friday 15 June 2018

So what IO do I need to do in the watcher?  ...

I don't like how the watcher extracts all the file names from the channel in one go, really.

Monday 18 June 2018

Started working on the master server now.  I think there is a flaw in my approach.  At the moment, I have all these requests coming in, and I am queuing them up and letting them wait while a single process works them out.  This is good in one way, because there is no locking, because all the state is managed by a single process, but the bad thing is that this single thread could be a bottleneck on the process.

The trouble with firing off a separate goroutine for every request is that the requests then have to modify the state, and how do you merge everything back in again afterwards?  There would have to be some locks, and that could well slow things down.  I suppose, though, that if it turned out that the 'update' function was slow, then since it is pure, I could just hand it ... Don't worry - I have a feeling it's going to be plenty fast enough - there is not much computation to do and it should be fine - if not, I can reshape it later.

The trouble now is that if a bunch of requests all come in at once, like requests A, B, and C in that order, then they will get put in the channel like this:

in -> [ C, B, A ] -> out

So if I shove in a request and wait for the response, then ...

I know, the request should contain a channel to respond on, then I know that the response will be the right one.

I'm now writing the 
